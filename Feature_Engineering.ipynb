{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering**\n",
    "\n",
    "## Key Features:\n",
    "- __Mechanical Ventilation Data Analysis__\n",
    "- __Organic Data Refactoring__\n",
    "- __Feature Engineering__\n",
    "- __Refactoring DataSets for Unsuperviced Learning Models__\n",
    "---\n",
    "\n",
    "## Maintainer\n",
    "\n",
    "- **GitHub**: [Molderon](https://github.com/Molderon)\n",
    "- **Email**: [Molderon@proton.me](mailto:Molderon@proton.me)\n",
    "- **ML Model at** [ICU: Mechanical Ventilation: Asyncronies Classification](https://github.com/Molderon/ICU-Mechanical-Ventilation-Asyncronies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Normal Respiratory hysteresis on **mechanical ventilation**\n",
    "- Mechanical Ventilator **<->** Patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./normalBreath2.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />\n",
    "<img src=\"./ExampleBr.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Asyncronous Respiratory Hysteresis** Examples:\n",
    "- Mechanical Ventilation **<->** Patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./undefined_async1.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />\n",
    "<img src=\"./doubleTrigger3.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />\n",
    "<img src=\"./failedtrigger.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />\n",
    "<img src=\"./failed_trigger1.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --<Environment Libs>--\n",
    "from dataclasses import dataclass, field\n",
    "import sys, os, subprocess  \n",
    "import pip, time\n",
    "\n",
    "# --<DataScience>--\n",
    "import seaborn \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.fft import fft\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# --<Feature Engineering>--\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import BSpline, splrep\n",
    "\n",
    "\n",
    "assert sys.version_info >= (3, 5)\n",
    "bScale: bool = False\n",
    "SingleSet: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./hyst_loop.jpg\" alt=\"Hysteresis P/V Loop\" style=\"opacity: 0.8;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setups\n",
    "plt.style.use('dark_background')\n",
    "csfont = {'fontname':'Comic Sans MS'}\n",
    "hfont = {'fontname':'Helvetica'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_LowFeq_sets():\n",
    "    os.chdir(\"/home/molderon/Main/WorkSpace/Diploma Project/Feature Engineering/\")\n",
    "    Scale_Dataset = StandardScaler()\n",
    "\n",
    "    try:\n",
    "        if(SingleSet == False):\n",
    "            Training_Set = pd.DataFrame()\n",
    "            MultiCalss_validation = pd.DataFrame()\n",
    "            CrossValidation = pd.DataFrame()\n",
    "\n",
    "            Training_Set = pd.read_csv(\"TrainingSet.csv\")\n",
    "            MultiCalss_validation = pd.read_csv(\"MultiValid.csv\")\n",
    "            CrossValidation = pd.read_csv(\"CrossValid.csv\")\n",
    "\n",
    "            # Renaming bad naming concentions\n",
    "            Training_Set.columns = Training_Set.columns.str.replace(\"u_in\", \"TV-i\")\n",
    "            Training_Set.columns = Training_Set.columns.str.replace(\"u_out\", \"TV-e\")\n",
    "            MultiCalss_validation.columns = MultiCalss_validation.columns.str.replace(\"u_in\", \"TV-i\")\n",
    "            MultiCalss_validation.columns = MultiCalss_validation.columns.str.replace(\"u_out\", \"TV-e\")\n",
    "            CrossValidation.columns = CrossValidation.columns.str.replace(\"u_in\", \"TV-i\")\n",
    "            CrossValidation.columns = CrossValidation.columns.str.replace(\"u_out\", \"TV-e\")\n",
    "\n",
    "            #Droping useless weight\n",
    "            Training_Set.dropna()\n",
    "            MultiCalss_validation.dropna()\n",
    "            CrossValidation.dropna()\n",
    "\n",
    "            Training_Set = Training_Set.dropna(subset=['pressure'])\n",
    "            MultiCalss_validation = MultiCalss_validation.dropna(subset=['pressure'])\n",
    "            CrossValidation = CrossValidation.dropna(subset=['pressure'])\n",
    "\n",
    "            Training_Set.drop(Training_Set.columns[[0,1]], axis =1, inplace=True)\n",
    "            CrossValidation.drop(CrossValidation.columns[[0,1]], axis =1, inplace=True)\n",
    "            MultiCalss_validation.drop(MultiCalss_validation.columns[[0,1]], axis =1, inplace=True)\n",
    "\n",
    "            # Optional Scaling\n",
    "\n",
    "            if bScale == True:\n",
    "                Training_Set = Scale_Dataset(Training_Set)\n",
    "                MultiCalss_validation = Scale_Dataset(MultiCalss_validation)\n",
    "                CrossValidation = Scale_Dataset(CrossValidation)\n",
    "\n",
    "            print(Training_Set.describe())\n",
    "            return Training_Set, MultiCalss_validation, CrossValidation\n",
    "\n",
    "        else:\n",
    "            Full_Dataset = pd.DataFrame()\n",
    "            Full_Dataset = pd.read_csv(\"Default_MV_Data.csv\")\n",
    "\n",
    "            Full_Dataset.columns = Full_Dataset.columns.str.replace(\"u_in\", \"TV-i\")\n",
    "            Full_Dataset.columns = Full_Dataset.columns.str.replace(\"u_out\", \"TV-e\")\n",
    "            Full_Dataset.drop(Full_Dataset.columns[[0,1]], axis =1, inplace=True)\n",
    "            Full_Dataset.dropna()\n",
    "\n",
    "            if bScale == True:\n",
    "                Full_Dataset = Scale_Dataset(Full_Dataset)\n",
    "\n",
    "            return Full_Dataset\n",
    "        \n",
    "    except Exception:\n",
    "        print(Exception.with_traceback())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'R'}>, <Axes: title={'center': 'C'}>],\n",
       "       [<Axes: title={'center': 'time_step'}>,\n",
       "        <Axes: title={'center': 'TV-i'}>],\n",
       "       [<Axes: title={'center': 'TV-e'}>,\n",
       "        <Axes: title={'center': 'pressure'}>]], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Training_Set.iloc[:,1:].hist(bins = 50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># ⚠️ **Conditional Warning: Time-Intensive Operation**\n",
    "> **Condition: Datasets larger than: 300 Mbytes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Draw_ScatterMatrix(Dataset: pd.DataFrame):    \n",
    "    set_attributes: list = [\"pressure\", \"TV-e\", \"TV-i\",\"R\", \"C\", \"time_step\"]\n",
    "    scatter_matrix(Dataset[set_attributes], figsize=(12,8))\n",
    "\n",
    "Main_Data = Load_LowFeq_sets()\n",
    "Draw_ScatterMatrix(Main_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./air_plot.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Matrix of organic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Scatter_Matrix.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> |Open Questions\n",
    "- Q: What is the data frequency of an individual Breath ?\n",
    "- A: Frequency ​≈ 26.67 Hz\n",
    "- Q: Does the timestep for individual readings remain **consistant** ?\n",
    "- A: Not consistant, it's voletile between the ranges **(32 - 40 miliseconds)**\n",
    "\n",
    "## Conclusions:\n",
    "- Avg. breath cycle is 2.7 sec = **~ (80 * 7)** datapoints per breath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> |Feature Engineering\n",
    "- Feature **A)** Reconstruction of sythetic datapoints: **Increase frequiency to ~ (32)Hz/sec.**\n",
    "- Feature **B)** Create a custom **Hysteresis Cycle Metric** for each breath id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Interpolation of each Dataset\n",
    "- Training Set\n",
    "- MultiClass_validation Set\n",
    "- Corss-Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Example_Dataset(DataSet: pd.DataFrame):\n",
    "   \n",
    "    ExampleSet = pd.DataFrame()\n",
    "    gss = GroupShuffleSplit(test_size=0.995, random_state=42) # loads 30% of Given Dataset\n",
    "    for idx, _ in gss.split(DataSet, groups=DataSet['breath_id']):\n",
    "        ExampleSet = DataSet.iloc[idx]\n",
    "    return ExampleSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '0' == 2x the dataset Frequiency\n",
    "# 'n' == 2x+n the dataset Frequency\n",
    "Interloper: int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reconstruct_Cycle(DataSet: pd.DataFrame, Single_Cycle: pd.DataFrame, Interloper: int):\n",
    "    # Define features and target\n",
    "    X_features = Single_Cycle[['time_step', 'R', 'C', 'TV-i', 'TV-e']].values\n",
    "    Y_target = Single_Cycle['pressure'].values\n",
    "\n",
    "    # Calculate synthetic time steps with desired number of points (interpolation count)\n",
    "    Interloper = min(Interloper + Single_Cycle.shape[0], 25)  # Cap at  if necessary\n",
    "    Synthetic_Timestep = np.linspace(Single_Cycle['time_step'].min(), Single_Cycle['time_step'].max(), Interloper)\n",
    "\n",
    "    # Spline for Pressure\n",
    "    try:\n",
    "        Symetra = splrep(Single_Cycle['time_step'], Y_target, s=10)\n",
    "        Pressure_Prediction = BSpline(*Symetra)(Synthetic_Timestep)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in pressure interpolation: {e}\")\n",
    "        return DataSet\n",
    "\n",
    "    # Inner function for variable reconstruction with adjustable smoothing factor\n",
    "    def reconstruct_variable(target_column, smooth_factor=1):\n",
    "        Y_target = Single_Cycle[target_column].values\n",
    "        try:\n",
    "            tck = splrep(Single_Cycle['time_step'], Y_target, s=smooth_factor)\n",
    "            return BSpline(*tck)(Synthetic_Timestep)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {target_column} interpolation: {e}\")\n",
    "            return np.full(Synthetic_Timestep.shape, np.nan)\n",
    "\n",
    "    # Reconstruct each variable with consistent smoothing\n",
    "    TV_i_pred = reconstruct_variable('TV-i')\n",
    "    TV_e_pred = reconstruct_variable('TV-e')\n",
    "    R_pred = reconstruct_variable('R')\n",
    "    C_pred = reconstruct_variable('C')\n",
    "\n",
    "    # Create DataFrame with interpolated data for the single cycle\n",
    "    breath_id = Single_Cycle['breath_id'].iloc[0]\n",
    "    Interpolated_Data = pd.DataFrame({\n",
    "        'breath_id': np.full(Synthetic_Timestep.shape, breath_id),\n",
    "        'R': R_pred,\n",
    "        'C': C_pred,\n",
    "        'time_step': Synthetic_Timestep,\n",
    "        'TV-i': TV_i_pred,\n",
    "        'TV-e': TV_e_pred,\n",
    "        'pressure': Pressure_Prediction\n",
    "    })\n",
    "\n",
    "    # Append the interpolated cycle data to DataSet\n",
    "    DataSet = pd.concat([DataSet, Interpolated_Data], ignore_index=True)\n",
    "    \n",
    "    return DataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reconstruct_Dataset(DataSet: pd.DataFrame, dataset_name: str, Interloper: int) -> pd.DataFrame:\n",
    "    for breath_id, group in DataSet.groupby('breath_id'):\n",
    "        Single_Cycle = group.sort_values(by='time_step').copy()\n",
    "        \n",
    "        DataSet = Reconstruct_Cycle(DataSet, Single_Cycle, Interloper)\n",
    "\n",
    "    DataSet.to_csv(dataset_name)\n",
    "    return DataSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Interpolation\n",
    " **Default Datapoints (Left) - 16 Hz/sec**   |   **Interpolation (Right) - 32 Hz/sec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interpolation():\n",
    "      import time,warnings\n",
    "      # Suppress specific Pandas warnings\n",
    "      warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "      warnings.filterwarnings(\"ignore\")\n",
    "      Boosted_Dataset = pd.DataFrame()\n",
    "      start = time.time()\n",
    "      pd.options.mode.chained_assignment = None\n",
    "      \n",
    "      DataSet = Reconstruct_Dataset(Load_LowFeq_sets(),\"Boosted_MV.csv\", Interloper)\n",
    "      \n",
    "      \n",
    "      ''' //Example for splitted sets\n",
    "            Boosted_TrainingSet = Reconstruct_Dataset(Training_Set,\"Boosted_TrainingSet.csv.csv\", Interloper)\n",
    "            Boosted_MultiClass =  Reconstruct_Dataset(CrossValidation,\"Boosted_MultiClass.csv\", Interloper)\n",
    "            Boosted_CrossValidation = Reconstruct_Dataset(MultiCalss_validation,\"Boosted_CrossValidation.csv\", Interloper)\n",
    "      '''\n",
    "      \n",
    "      end = time.time()\n",
    "      print(\"Total time for interpolations is :\",\n",
    "            (end-start)/60, \"Min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># ⚠️ **Warning: Time-Intensive Operation**\n",
    ">\n",
    "> The interpolation process you're about to run is computationally expensive and may take **several hours** to complete, the author advices to use small sliced datasets.\n",
    "># **Beware**\n",
    ">\n",
    "> - Ensure that your system has sufficient resources (CPU/RAM) to handle the process.\n",
    "> - Avoid interrupting the process once it has started.\n",
    "> - If possible, run this operation on a machine that will not be needed for other tasks during execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for interpolations is : 15.2435875137647 Min\n"
     ]
    }
   ],
   "source": [
    "Interpolation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation Description:\n",
    "\n",
    "\n",
    "1. **Input Features**: \n",
    "   $$\n",
    "   X = [\\text{time\\_step}, R, C, TV_{i}, TV_{e}] \\in \\mathbb{R}^{n \\times m}\n",
    "   $$\n",
    "   where \\(n\\) is the number of samples and \\(m\\) is the number of features.\n",
    "\n",
    "   - \\(Y\\) is defined as the target variable:\n",
    "   $$\n",
    "   Y = \\text{pressure} \\in \\mathbb{R}^{n}\n",
    "   $$\n",
    "\n",
    "2. **Synthetic Time Steps**: \n",
    "   - The number of interpolated points  :\n",
    "   $$\n",
    "   n_{\\text{interloper}} = n + I\n",
    "   $$\n",
    "   where \\(I\\) is the additional points specified by the user.\n",
    "\n",
    "   - A synthetic time step vector is created:\n",
    "   $$\n",
    "   t_{\\text{synthetic}} = \\text{linspace}(t_{\\text{min}}, t_{\\text{max}}, n_{\\text{interloper}})\n",
    "   $$\n",
    "\n",
    "3. **Pressure Prediction Using B-Splines**: \n",
    "   - The B-spline representation of the pressure is generated using the `splrep` function:\n",
    "   $$\n",
    "   tck_{\\text{pressure}} = \\text{splrep}(S[\\text{time\\_step}], Y, s=0)\n",
    "   $$\n",
    "   - The predicted pressure values at synthetic time steps are calculated as:\n",
    "   $$\n",
    "   P_{\\text{pred}} = BSpline(tck_{\\text{pressure}})(t_{\\text{synthetic}})\n",
    "   $$\n",
    "\n",
    "4. **Variable Reconstruction**:\n",
    "   - A nested function is defined to reconstruct other variables in the dataset:\n",
    "   $$\n",
    "   V_{\\text{pred}} = BSpline(tck_{\\text{variable}})(t_{\\text{synthetic}})\n",
    "   $$\n",
    "\n",
    "   - This is computed as:\n",
    "   $$\n",
    "   tck_{\\text{variable}} = \\text{splrep}(S[\\text{time\\_step}], S[\\text{variable}], s=3)\n",
    "   $$\n",
    "\n",
    "5. **Constructing the Output DataFrame**: \n",
    "   - Finally, an interpolated dataset \\(I\\) is created:\n",
    "   $$\n",
    "   I = \\begin{bmatrix}\n",
    "   \\text{breath\\_id} & C_{\\text{pred}} & t_{\\text{synthetic}} & TV_{i_{\\text{pred}}} & TV_{e_{\\text{pred}}} & P_{\\text{pred}} \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   - The final reconstructed dataset is obtained by concatenating the original dataset \\(D\\) with the interpolated data \\(I\\):\n",
    "   $$\n",
    "   D_{\\text{new}} = D \\cup I\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Hysteresis(Hysteresis_Dynamics: pd.DataFrame):\n",
    "    TV_i = np.array(Hysteresis_Dynamics['TV-i'])\n",
    "    TV_e = np.array(Hysteresis_Dynamics['TV-e'])\n",
    "    pressure = np.array(Hysteresis_Dynamics['pressure'])\n",
    "\n",
    "    TV_total = TV_i + TV_e\n",
    "\n",
    "    plt.ion() \n",
    "    fig, ax = plt.subplots()\n",
    "    line, = ax.plot([], [], marker='o', linestyle='-', color='b', label='Hysteresis (TV-total / Pressure)')\n",
    "\n",
    "    ax.set_title('Hysteresis Loop: TV-total/Pressure - 35.0 Hz')\n",
    "    ax.set_xlabel('TV-total (TV-i + TV-e)')\n",
    "    ax.set_ylabel('Pressure')\n",
    "    ax.legend()\n",
    "\n",
    "    def update_plot(x_data, y_data):\n",
    "        line.set_data(x_data, y_data)\n",
    "        ax.relim()          \n",
    "        ax.autoscale_view() \n",
    "        fig.canvas.draw()   \n",
    "        fig.canvas.flush_events() \n",
    "\n",
    "    for i in range(len(TV_total)):\n",
    "        update_plot(TV_total[:i+1], pressure[:i+1])\n",
    "\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Draw_Reconstructed_Cycles(breath_id: int, DataSet: pd.DataFrame):\n",
    "    Random_ids = pd.DataFrame()\n",
    "    Random_ids = DataSet[DataSet['breath_id'] == breath_id].copy()\n",
    "    Random_ids = Random_ids.sort_values(by=\"time_step\")\n",
    "\n",
    "    Hysteresis_Dynamics: dict = field(default_factory=dict)\n",
    "    target_columns = [\"TV-i\", \"TV-e\",\"pressure\", \"R\", \"C\", \"time_step\"]\n",
    "\n",
    "    Hysteresis_Dynamics = Random_ids[target_columns].to_dict(orient ='list')\n",
    "    Plot_Hysteresis(Hysteresis_Dynamics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Comparison\n",
    ">  - Left(26Hz) - Default\n",
    "\n",
    "> - Right(32Hz) - Interpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./17Hz.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />\n",
    "<img src=\"./32Hz.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># Polynomial Hysteresis Area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hysteresis_Area_Metric(DataSet: pd.DataFrame):\n",
    "    os.chdir(\"/home/molderon/Main/WorkSpace/Diploma Project/Classification Algorithms/\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if(SingleSet == False):\n",
    "        Boosted_TrainingSet = Feature_Engineering(Boosted_TrainingSet)\n",
    "        Boosted_TrainingSet.to_csv(\"Cluster_TrainingSet.csv\")\n",
    "\n",
    "        Boosted_MultiClass = Feature_Engineering(Boosted_MultiClass)\n",
    "        Boosted_MultiClass.to_csv(\"Cluster_MultiClass.csv\")\n",
    "\n",
    "        Boosted_CrossValidation= Feature_Engineering(Boosted_CrossValidation)\n",
    "        Boosted_CrossValidation.to_csv(\"Cluster_CrossValid.csv\")\n",
    "    else:\n",
    "        DataSet = Feature_Engineering(DataSet)\n",
    "        return DataSet\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Execution time:\", (end_time - start_time)/60, \"::mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Feature Engineering\n",
    "- **Pressure/Volume Loop** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# |Creating P/V cycles as a feature\n",
    "- **Working with the Interloped Datasets _~(32Hz)~_**\n",
    "\n",
    "- **The area of the polygon can be calculated using the formula:**\n",
    "\n",
    "$$\n",
    "A = \\frac{1}{2} \\left| \\sum_{i=1}^{n} \\left( x_i \\cdot y_{i+1} - y_i \\cdot x_{i+1} \\right) \\right|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(x_i\\) and \\(y_i\\) are the coordinates of the polygon's vertices.\n",
    "- The indices wrap around, i.e., \\(y_{n+1} = y_1\\) and \\(x_{n+1} = x_1\\).\n",
    "> __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "### Area Calculation Using the Trapezoidal Rule\n",
    "\n",
    "The area under a curve defined by discrete points can be calculated using the **trapezoidal rule**, which approximates the integral of the function. Given a set of points \\((x_i, y_i)\\), the area \\(A\\) under the curve from \\(x_1\\) to \\(x_n\\) can be represented mathematically as:\n",
    "\n",
    "$$\n",
    "A = \\int_{a}^{b} f(x) \\, dx \\approx \\sum_{i=1}^{n-1} \\frac{(y_i + y_{i+1})}{2} (x_{i+1} - x_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( A \\) is the approximate area under the curve.\n",
    "- \\( n \\) is the number of discrete points.\n",
    "- \\( x_i \\) are the \\( x \\)-coordinates of the points.\n",
    "- \\( y_i \\) are the \\( y \\)-coordinates of the points.\n",
    "- The term \\(\\frac{(y_i + y_{i+1})}{2}\\) represents the average height of the function between \\(x_i\\) and \\(x_{i+1}\\).\n",
    "- The difference \\((x_{i+1} - x_i)\\) represents the width of the interval.\n",
    "\n",
    "In the context of the provided code:\n",
    "**Area Calculation**:\n",
    "   The area is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{area} = |A| = \\left| \\int_{x_{\\text{closed}}} y_{\\text{closed}} \\, dx \\right| \\approx \\text{np.abs}\\left(\\text{np.trapz}(y_{\\text{closed}}, x_{\\text{closed}})\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Terraform_DataSet(Hysteresis_Sample: pd.DataFrame):\n",
    "\n",
    "    Tidal_Volume = Hysteresis_Sample['TV-i'].values + Hysteresis_Sample['TV-e'].values\n",
    "    Apparent_Pressure = Hysteresis_Sample[\"pressure\"].values\n",
    "\n",
    "    Polynomial_Area = np.abs(np.trapz(Apparent_Pressure, Tidal_Volume))\n",
    "\n",
    "    Hull_Data = np.column_stack((Tidal_Volume, Apparent_Pressure))\n",
    "    Hull = ConvexHull(Hull_Data)\n",
    "    Hull_Area = Hull.volume\n",
    "\n",
    "    ft_Tidal_Volume = np.abs(fft(Tidal_Volume))[:5]\n",
    "    ft_Pressure = np.abs(fft(Apparent_Pressure))[:5]\n",
    "\n",
    "    features = {\n",
    "        \"Polynomial_Area\": Polynomial_Area,\n",
    "        \"Hull_Area\": Hull_Area,\n",
    "        **{f\"ft_Tidal_Volume_{i+1}\": coef for i, coef in enumerate(ft_Tidal_Volume)},\n",
    "        **{f\"ft_Pressure_{i+1}\": coef for i, coef in enumerate(ft_Pressure)}\n",
    "    }\n",
    "\n",
    "    features_df = pd.DataFrame(features, index=[0])\n",
    "    enhanced_Hysteresis_Sample = pd.concat([Hysteresis_Sample.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "    return enhanced_Hysteresis_Sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Feature_Engineering(DataSet: pd.DataFrame):\n",
    "    DataSet = DataSet.sort_values(by=['breath_id', 'time_step']).copy()\n",
    "    Terraformed_Data = []\n",
    "\n",
    "    for breath_id, group in DataSet.groupby('breath_id'):\n",
    "        Hysteresis_Sample = Terraform_DataSet(group)  \n",
    "        Terraformed_Data.append(Hysteresis_Sample)\n",
    "\n",
    "    DataSet = pd.concat(Terraformed_Data, ignore_index=True)\n",
    "\n",
    "    return DataSet\n",
    "\n",
    "\n",
    "def Clean_DataSet(DataSet: pd.DataFrame):\n",
    "    DataSet.dropna(subset=['pressure', 'Polynomial_Area'], inplace=True)\n",
    "    DataSet = DataSet[DataSet['Polynomial_Area'] <= 417]\n",
    "    DataSet = DataSet[~((DataSet['Polynomial_Area'] > 73) & (DataSet['Polynomial_Area'] < 150))]\n",
    "    DataSet.drop(DataSet.columns[[0,2,3]], axis =1, inplace=True)\n",
    "    # Within These ranges The individual Breath cycles do not connect or are incomplete\n",
    "    return DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./HeatMap.png\" alt=\"Scatter Matrix\" style=\"opacity: 1;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# |Cleaning The DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3067600 entries, 0 to 6035999\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   breath_id        float64\n",
      " 1   time_step        float64\n",
      " 2   TV-i             float64\n",
      " 3   TV-e             float64\n",
      " 4   pressure         float64\n",
      " 5   Polynomial_Area  float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 163.8 MB\n"
     ]
    }
   ],
   "source": [
    "DataSet = Clean_DataSet(DataSet)\n",
    "DataSet.info()\n",
    "DataSet.to_csv(\"ClusterSet_26Hz.csv\")\n",
    "del DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Machine Learning Starts Soon :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Converging data for **Unsupervised Learning**\n",
    "-  a.k.a **Theoretical Rocket Surgery**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet = Feature_Engineering(DataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38345"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_ClusterSet = DataSet.drop_duplicates(subset='breath_id', keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF_ClusterSet.drop(TF_ClusterSet.columns[[1,2,3,4]], axis =1, inplace=True)\n",
    "#TF_ClusterSet.head()\n",
    "TF_ClusterSet.to_csv(\"TF_Cluster26Hz.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
